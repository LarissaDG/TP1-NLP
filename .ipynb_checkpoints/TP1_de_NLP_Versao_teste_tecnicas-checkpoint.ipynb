{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1>First Assignment</h1>\n",
        "<h2>Word embeddings</h2>\n",
        "<h3>Intrinsic evaluation</h3>\n",
        "\n",
        "------------------------------------------\n",
        "Aluna: Larissa Dolabella Gomide <br>\n",
        "Matrícula: 2022685185\n",
        "\n",
        "------------------------------------------\n",
        "● Objective:<br>\n",
        "○ Prepare data for modeling language<br>\n",
        "○ Learn a word2vec language model<br>\n",
        "○ Evaluate the model<br>\n",
        "Intrinsic evaluation<br>\n",
        "<br>\n",
        "● Code:<br>\n",
        "○ gensym (there are other implementations)<br><br>\n",
        "\n",
        "● Corpus:<br>\n",
        "○ https://mattmahoney.net/dc/text8.zip (there are other corpora)<br><br>\n",
        "\n",
        "● Pre-processing:<br>\n",
        "○ Punctuation, lower case, etc.<br>\n",
        "<br>\n",
        "\n",
        "● Choices:<br>\n",
        "○ training sizes, window sizes, CBOW vs Skip-Gram etc.<br><br>\n",
        "\n",
        "● Evaluation:<br>\n",
        "○ Analogies using https://github.com/nicholas-leonard/word2vec/blob/master/questions-words.txt<br><br>\n",
        "\n",
        "■ input three words, pick the returned word, compute the distance to the correct word<br>\n",
        "■ Repeat and average<br>"
      ],
      "metadata": {
        "id": "Nrqy-KEcqlMn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------------------"
      ],
      "metadata": {
        "id": "1O2vHP37yfqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Bibliotecas\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.word2vec import LineSentence\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim import corpora\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px"
      ],
      "metadata": {
        "id": "APGjEJNWyNZV"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Baixando os arquivos"
      ],
      "metadata": {
        "id": "MG8jpeCUyaxF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcX-Rzg0qT7n",
        "outputId": "4fadbc63-9875-4e68-d72e-289625d37363"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-04 21:11:31--  https://mattmahoney.net/dc/text8.zip\n",
            "Resolving mattmahoney.net (mattmahoney.net)... 34.198.1.81\n",
            "Connecting to mattmahoney.net (mattmahoney.net)|34.198.1.81|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 31344016 (30M) [application/zip]\n",
            "Saving to: ‘text8.zip.1’\n",
            "\n",
            "text8.zip.1         100%[===================>]  29.89M  29.6MB/s    in 1.0s    \n",
            "\n",
            "2023-11-04 21:11:32 (29.6 MB/s) - ‘text8.zip.1’ saved [31344016/31344016]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://mattmahoney.net/dc/text8.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip text8.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tD7PDErSyBxY",
        "outputId": "3bb2e060-39cb-4fb3-8bb4-7dd7d6e407d0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  text8.zip\n",
            "replace text8? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------------------------------"
      ],
      "metadata": {
        "id": "jxBN4BN3yhuk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> Pré - Processamento </h2>\n",
        "\n",
        "O arquivo text8 consiste de uma unica sentença que compõe todo o Corpus. Tentei algumas estratégias para abrir o arquivo e trabalhar com ele diretamente em python mas sem muito sucesso, como os métodos mencionados em https://www.geeksforgeeks.org/nlp-gensim-tutorial-complete-guide-for-beginners/.\n",
        "\n",
        "Então optei por fazer uma parte do pré-processamento em C. É rápido e me permite transformar os dados em um arquivo mais fácil de trabalhar. Além disso, já removo eventuais pontuações e converto os textos para minúsculas."
      ],
      "metadata": {
        "id": "0crRRZroQysj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Alternativa ao C - Provavel. Mas não aguenta imprmir o 'tokenized' ou quando consegue imprime um vertor vazio. Por isso optei por usar C\n",
        "# open the text file as an object\n",
        "#doc = open('sample_data.txt', encoding ='utf-8')\n",
        "\n",
        "# preprocess the file to get a list of tokens\n",
        "#tokenized =[]\n",
        "#for sentence in doc.read().split('.'):\n",
        "  # the simple_preprocess function returns a list of each sentence\n",
        "  #tokenized.append(simple_preprocess(sentence, deacc = True))\n",
        "\n",
        "#print(tokenized)"
      ],
      "metadata": {
        "id": "A_xxBnNoOGmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instale o GCC (Compilador C)\n",
        "!apt-get update -qq\n",
        "!apt-get install -y build-essential"
      ],
      "metadata": {
        "id": "BdhEvA3OPomM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile preprocessing.c\n",
        "#include <stdlib.h>\n",
        "#include <stdio.h>\n",
        "#include <string.h>\n",
        "#include <ctype.h>\n",
        "\n",
        "void remove_pontuacao_minuscula(char palavra[]) {\n",
        "    int i, j = 0;\n",
        "    char nova_palavra[100];\n",
        "\n",
        "    for (i = 0; palavra[i] != '\\0'; i++) {\n",
        "        if (!ispunct(palavra[i])) { // Verifica se não é uma pontuação\n",
        "            nova_palavra[j] = tolower(palavra[i]); // Converte para minúsculas\n",
        "            j++;\n",
        "        }\n",
        "    }\n",
        "    nova_palavra[j] = '\\0'; // Adiciona o terminador de string\n",
        "\n",
        "    strcpy(palavra, nova_palavra); // Copia a nova palavra de volta para a palavra original\n",
        "}\n",
        "\n",
        "\n",
        "int main (){\n",
        "    FILE *arquivo;\n",
        "    FILE *output;\n",
        "\n",
        "    char palavra[100]; // Buffer para armazenar a palavra lida\n",
        "\n",
        "    arquivo = fopen(\"text8\", \"r\");\n",
        "\n",
        "    if (arquivo == NULL) {\n",
        "        printf(\"Não foi possível abrir o arquivo.\\n\");\n",
        "        return 1;\n",
        "    }\n",
        "\n",
        "    output = fopen(\"output.txt\", \"w\");\n",
        "    if (output == NULL) {\n",
        "        printf(\"Não foi possível abrir o arquivo.\\n\");\n",
        "        return 1;\n",
        "    }\n",
        "\n",
        "\n",
        "    // Lê uma palavra do arquivo\n",
        "    while (fscanf(arquivo, \"%s\", palavra) == 1){\n",
        "        //printf(\"%s\\t - \",palavra);\n",
        "        remove_pontuacao_minuscula(palavra);\n",
        "        //printf(\"\\tNova palavra %s\\n\",palavra);\n",
        "        fprintf(output, \"%s\\n\", palavra);\n",
        "    }\n",
        "\n",
        "\n",
        "    // Fecha o arquivo\n",
        "    fclose(arquivo);\n",
        "    fclose(output);\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "id": "L1vqeGFTPjWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile o programa C\n",
        "!gcc preprocessing.c\n",
        "\n",
        "# Execute o programa compilado\n",
        "!./a.out\n"
      ],
      "metadata": {
        "id": "tu7QX5VWPs47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-------------------------------------------------------------"
      ],
      "metadata": {
        "id": "Ew3BGjR8RnjZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the file path to the text8 corpus\n",
        "corpus_file = '/content/text8'"
      ],
      "metadata": {
        "id": "j-sOp2SlK_J6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to tokenize and preprocess the text\n",
        "def preprocess_text(text):\n",
        "    tokens = text.split()  # Tokenize by splitting on spaces\n",
        "    # You can add more preprocessing steps here, like lowercasing, removing punctuation, etc.\n",
        "    return tokens\n",
        "\n",
        "# Use LineSentence to read the corpus\n",
        "sentences = LineSentence(corpus_file)\n",
        "\n",
        "# Preprocess the sentences and create a list of tokenized sentences\n",
        "preprocessed_sentences = [preprocess_text(\" \".join(sentence)) for sentence in sentences]\n",
        "\n",
        "# Train the Word2Vec model using CBOW\n",
        "model = Word2Vec(preprocessed_sentences, sg=0, window=5, min_count=5, workers=4)\n",
        "\n",
        "# Save the trained model to a file\n",
        "model.save(\"word2vec_cbow.model\")\n",
        "\n",
        "# Now, you can use the trained Word2Vec model for various NLP tasks\n"
      ],
      "metadata": {
        "id": "gNfUPKfRLlSG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregar o modelo Word2Vec treinado\n",
        "model = Word2Vec.load(\"word2vec_cbow.model\")\n",
        "\n",
        "# Consulta de exemplo\n",
        "query_word = \"king\"\n",
        "\n",
        "# Encontre palavras semelhantes à palavra de consulta\n",
        "similar_words = model.wv.most_similar(query_word, topn=5)\n",
        "\n",
        "# Exiba as palavras semelhantes encontradas\n",
        "print(f\"Palavras semelhantes a '{query_word}':\")\n",
        "for word, score in similar_words:\n",
        "    print(f\"{word}: {score}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41SNZqajMk1q",
        "outputId": "a4f1b85f-b18a-4988-afa6-3db2ed2a8de7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Palavras semelhantes a 'king':\n",
            "prince: 0.7335668802261353\n",
            "queen: 0.7202160954475403\n",
            "throne: 0.7104078531265259\n",
            "kings: 0.7054188847541809\n",
            "emperor: 0.6967767477035522\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Pega os tokens do arquivo output.txt e armazena no vetor tokenized para fazer as operações necessárias\n",
        "tokenized =[]\n",
        "with open('output.txt', 'r') as arquivo:\n",
        "    linha = arquivo.readline()\n",
        "\n",
        "    while linha:\n",
        "        linha_processada = linha.strip()\n",
        "        tokenized.append(linha_processada)\n",
        "        linha = arquivo.readline()\n",
        "#print(tokenized)\n",
        "num_tokens = len(tokenized)\n",
        "print(num_tokens)"
      ],
      "metadata": {
        "id": "MFqVVS2PrBiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optou-se por fazer uma lematização, nos termos para evitar que pequenas variações na palavra mudem drasticamente a contagem de termos. A baixo vemos um exemplo de palavras similares mas que foram contadas separadamente.<br><br>\n",
        "![Captura de tela 2023-11-02 223045.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "NfVzdLBSzgYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto = ' '.join(tokenized) #Junta os tokens do texto para remover stopwords\n",
        "nltk.download('stopwords')  # Baixe as stopwords se ainda não as tiver\n",
        "nltk.download('wordnet') # Certifique-se de ter o pacote WordNet instalado\n",
        "\n",
        "# Remover stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens_sem_stopwords = [palavra for palavra in tokenized if palavra.lower() not in stop_words]\n",
        "\n",
        "# Crie um objeto lematizador\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "palavras_lematizadas = [lemmatizer.lemmatize(palavra) for palavra in tokens_sem_stopwords]\n",
        "\n",
        "# Contar a frequência das palavras\n",
        "frequencia_palavras = Counter(palavras_lematizadas) #Palavras e frequencias\n",
        "\n",
        "#print(\"\\nFrequência das palavras:\")\n",
        "#for palavra, frequencia in frequencia_palavras.items():\n",
        "    #print(f\"{palavra}: {frequencia}\")\n",
        "\n",
        "frequencias = list(frequencia_palavras.values()) #Vetor só de frequencias"
      ],
      "metadata": {
        "id": "b835sxsBs7pS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plota um boxplot mostrando a distribuição das frequencias das palavras no texto:\n",
        "fig = px.box(y=frequencias, title=\"Boxplot da Frequência das Palavras\")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "TNI77N_-vu41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtrar palavras com frequência maior que um threshold\n",
        "threshold = 3000\n",
        "frequencias_filtradas = [frequencia for frequencia in frequencias if frequencia > threshold]\n",
        "tokens_filtrados = [token for token, frequencia in frequencia_palavras.items() if frequencia > threshold]\n",
        "\n",
        "# Verificar os tokens e suas frequências\n",
        "for token, frequencia in zip(tokens_filtrados, frequencias_filtradas):\n",
        "    print(f\"{token}: {frequencia}\")"
      ],
      "metadata": {
        "id": "AC5ZhLLBxsg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Número de tokens mais frequentes: {len(tokens_filtrados)}\")\n",
        "print(f\"Número total de tokens: {num_tokens}\")\n",
        "print(f\"Proporção: {len(tokens_filtrados)/num_tokens}\")"
      ],
      "metadata": {
        "id": "Wvo42ucRRZGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cria uma lista de listas de tokens\n",
        "tokens_lematizados_lista = [[token] for token in palavras_lematizadas]\n",
        "tokens_filtrados_lista = [[token] for token in tokens_filtrados]\n",
        "\n",
        "# Criadicionários para as duas listas\n",
        "dictionary_1 = corpora.Dictionary(tokens_lematizados_lista)\n",
        "dictionary_2 = corpora.Dictionary(tokens_filtrados_lista)\n",
        "\n",
        "# Exibe os dicionários\n",
        "print(dictionary_1)\n",
        "print(dictionary_2)\n",
        "\n",
        "#Exibe o tamanho dos dicionários\n",
        "print(len(dictionary_1))\n",
        "print(len(dictionary_2))"
      ],
      "metadata": {
        "id": "tpuM0t9cvC0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------------------------------------------------------------"
      ],
      "metadata": {
        "id": "B32aSCk-J-YN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "'''O T-distributed Stochastic Neighbor Embedding (t-SNE) não faz parte da biblioteca Gensim.\n",
        "O t-SNE é um algoritmo de redução de dimensionalidade amplamente utilizado em aprendizado de\n",
        "máquina e visualização de dados para projetar dados de alta dimensionalidade em um espaço de baixa\n",
        "dimensionalidade, preservando as relações de proximidade entre os pontos.'''\n",
        "\n",
        "# Seus dados de alta dimensão\n",
        "dados = ...\n",
        "\n",
        "# Inicialize o modelo t-SNE\n",
        "tsne = TSNE(n_components=2, random_state=0)\n",
        "\n",
        "# Ajuste o modelo aos dados e obtenha os dados projetados no espaço 2D\n",
        "dados_projetados = tsne.fit_transform(dados)\n",
        "\n",
        "# Plote os dados projetados em um gráfico de dispersão\n",
        "plt.scatter(dados_projetados[:, 0], dados_projetados[:, 1])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "h673s8g-0iRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pGCxs3Ip2mr-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}